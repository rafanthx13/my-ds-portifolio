# An Introduction to Statistical Learning: with Applications in Python

**Slides, JupyterNotebook Data set e outros resources do livro**

==> https://www.statlearning.com/resources-python

**Organization of This Book**

Chapter 2 introduces the basic terminology and concepts behind statistical learning. This chapter also presents the K-nearest neighbor classifer, a
very simple method that works surprisingly well on many problems. Chapters 3 and 4 cover classical linear methods for regression and classifcation.
In particular, Chapter 3 reviews linear regression, the fundamental starting point for all regression methods. In Chapter 4 we discuss two of the
most important classical classifcation methods, logistic regression and linear discriminant analysis.

A central problem in all statistical learning situations involves choosing
the best method for a given application. Hence, in Chapter 5 we introduce cross-validation and the bootstrap, which can be used to estimate the
accuracy of a number of diferent methods in order to choose the best one.

Much of the recent research in statistical learning has concentrated on
non-linear methods. However, linear methods often have advantages over
their non-linear competitors in terms of interpretability and sometimes also
accuracy. Hence, in Chapter 6 we consider a host of linear methods, both
classical and more modern, which ofer potential improvements over standard linear regression. These include stepwise selection, ridge regression,
principal components regression, and the lasso.

The remaining chapters move into the world of non-linear statistical
learning. We frst introduce in Chapter 7 a number of non-linear methods that work well for problems with a single input variable. We then
show how these methods can be used to ft non-linear additive models for
which there is more than one input. In Chapter 8, we investigate tree-based
methods, including bagging, boosting, and random forests. Support vector
machines, a set of approaches for performing both linear and non-linear
classifcation, are discussed in Chapter 9. We cover deep learning, an approach for non-linear regression and classifcation that has received a lot
of attention in recent years, in Chapter 10. Chapter 11 explores survival
analysis, a regression approach that is specialized to the setting in which
the output variable is censored, i.e. not fully observed.

In Chapter 12, we consider the unsupervised setting in which we have
input variables but no output variable. In particular, we present principal components analysis, K-means clustering, and hierarchical clustering.
Finally, in Chapter 13 we cover the very important topic of multiple hypothesis testing.

**Páginas por capítulo**

01 - Introduction = 1 - 15 => 14 páginas
02 - Statistical Learning = 15 - 69 => 54 páginas
03 - Liner Regression = 69 - 135 => 66 páginas
04 - Classification = 135 - 201 => 66 páginas
05 - Resampling Methods = 201 - 229 => 28 páginas
06 - Linear Model Selection and Regularization = 229 - 289 => 60 páginas
07 - Moving Beyond Linearity = 289 - 331 => 42 páginas
08 - Tree-Baes Methods = 331 - 363 => 32 páginas
09 - Suport Vector Machines = 367 - 399 => 32 páginas
10 - Deep Learning = 399 - 469 = 70 páginas
11 - Survival Analysis and Censored Data = 469 - 498 => 29 páginas
12 - Unsupervised Learning = 503 - 552 => 49 páginas
13 - Multiple Testing = 557 - 593 => 36 páginas
